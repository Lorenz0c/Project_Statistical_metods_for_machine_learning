{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "confront_best.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTj57lkk3_E4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqIikhg_4JIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "#Download the dataset\n",
        "URL='https://github.com/Horea94/Fruit-Images-Dataset/archive/master.zip'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('master.zip', origin=URL, extract=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evuIiHed4LJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'Fruit-Images-Dataset-master')\n",
        "\n",
        "train_dir = os.path.join(PATH, 'Training')\n",
        "validation_dir = os.path.join(PATH, 'Test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNe8mok94PBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Select the classes we are interested in.\n",
        "dir = pathlib.Path(validation_dir)\n",
        "\n",
        "CLASS_NAMES = np.array([item.name for item in dir.glob('*')])\n",
        "\n",
        "reg=re.compile('^(Apple|Banana|Plum|Pepper|Cherry|Grape|Tomato|Potato|Pear|Peach).*')\n",
        "\n",
        "temporary_class_names=[]\n",
        "\n",
        "for c in CLASS_NAMES:\n",
        "  if reg.match(c):\n",
        "    temporary_class_names.append(c)\n",
        "\n",
        "CLASS_NAMES_MOST_USED=temporary_class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz0ZJa2N4UUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 50\n",
        "IMG_HEIGHT = 32\n",
        "IMG_WIDTH = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlsguu6e4VIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a dataset containing the training set we are intrested in.\n",
        "list_ds=[]\n",
        "\n",
        "for c in CLASS_NAMES_MOST_USED:\n",
        "  train_dirs = os.path.join(train_dir, c)\n",
        "  train_dirs = pathlib.Path(train_dirs)\n",
        "  list_ds.append(tf.data.Dataset.list_files(str(train_dirs/'*')))\n",
        "\n",
        "train_ds=list_ds[0].concatenate(list_ds[1])\n",
        "for i in range(2,len(list_ds)):\n",
        "  train_ds=train_ds.concatenate(list_ds[i])\n",
        "\n",
        "#Count the number of example in the training set.\n",
        "num=0\n",
        "for j in train_ds.as_numpy_iterator():\n",
        "  num=num+1\n",
        "TRAINING_NUMBER=num\n",
        "\n",
        "\n",
        "#create a dataset containing the test set we are intrested in.\n",
        "list_ds=[]\n",
        "\n",
        "for c in CLASS_NAMES_MOST_USED:\n",
        "  test_dirs = os.path.join(validation_dir, c)\n",
        "  test_dirs = pathlib.Path(test_dirs)\n",
        "  list_ds.append(tf.data.Dataset.list_files(str(test_dirs/'*')))\n",
        "\n",
        "test_ds=list_ds[0].concatenate(list_ds[1])\n",
        "for i in range(2,len(list_ds)):\n",
        "  test_ds=test_ds.concatenate(list_ds[i])\n",
        "\n",
        "#Count the number of example in the test set.\n",
        "num=0\n",
        "for j in test_ds.as_numpy_iterator():\n",
        "  num=num+1\n",
        "TEST_NUMBER=num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu58O8hS4bmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#return the label associated to the image.\n",
        "def decode_label(parts):\n",
        "\n",
        "  if tf.strings.regex_full_match(parts, '.*Apple.*'):\n",
        "    return tf.constant(0)\n",
        "  if tf.strings.regex_full_match(parts, '.*Banana.*'):\n",
        "    return tf.constant(1)\n",
        "  if tf.strings.regex_full_match(parts, '.*Plum.*'):\n",
        "    return tf.constant(2)\n",
        "  if tf.strings.regex_full_match(parts, '.*Pepper.*'):\n",
        "    return tf.constant(3)\n",
        "  if tf.strings.regex_full_match(parts, '.*Cherry.*'):\n",
        "    return tf.constant(4)\n",
        "  if tf.strings.regex_full_match(parts, '.*Grape.*'):\n",
        "    return tf.constant(5)\n",
        "  if tf.strings.regex_full_match(parts, '.*Tomato.*'):\n",
        "    return tf.constant(6)\n",
        "  if tf.strings.regex_full_match(parts, '.*Potato.*'):\n",
        "    return tf.constant(7)\n",
        "  if tf.strings.regex_full_match(parts, '.*Pear.*'):\n",
        "    return tf.constant(8)\n",
        "  if tf.strings.regex_full_match(parts, '.*Peach.*'):\n",
        "    return tf.constant(9)\n",
        "  \n",
        "  return tf.constant(-1)\n",
        "\n",
        "#return the image.\n",
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  # resize the image.\n",
        "  return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
        "\n",
        "# return the image and the associated label given the file path.\n",
        "def process_path(file_path):\n",
        "  label = decode_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diS2qPs64eqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "testing_ds = test_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# Shuffle the images in the training set, and divide the images in the two sets in batch.\n",
        "ds_for_training=training_ds.shuffle(TRAINING_NUMBER).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
        "ds_for_test=testing_ds.batch(TEST_NUMBER).prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIzzRxHm4hU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compute the error of a predictor on a dataset using the zero one loss function.\n",
        "def zero_one_loss(model,ds):\n",
        "  probability_model = tf.keras.Sequential([model, \n",
        "                                         tf.keras.layers.Softmax()])\n",
        "\n",
        "  images,labels=next(iter(ds))\n",
        "  predictions = probability_model.predict(images)\n",
        "\n",
        "  zero_one_loss=0\n",
        "  for i in range(0,len(predictions)):\n",
        "    if labels[i]!=np.argmax(predictions[i]):\n",
        "      zero_one_loss=zero_one_loss+1\n",
        "\n",
        "  res=zero_one_loss/(len(predictions))\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztXwzmpr4oy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc1=[]\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(32, 32 ,3)),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "for i in range(5):\n",
        "  model.fit(x=ds_for_training,\n",
        "            epochs=1)\n",
        "  \n",
        "  test_loss=zero_one_loss(model,ds_for_test)\n",
        "\n",
        "  acc1.append(test_loss)\n",
        "  print('epoch: ',i+1)\n",
        "  print(test_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVl6RhyzKjos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc2=[]\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(32, 32 ,3)),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "for i in range(5):\n",
        "  model.fit(x=ds_for_training,\n",
        "            epochs=1)\n",
        "  \n",
        "  test_loss=zero_one_loss(model,ds_for_test)\n",
        "\n",
        "  acc2.append(test_loss)\n",
        "  print('epoch: ',i+1)\n",
        "  print(test_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6gllrrK5JCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc3=[]\n",
        "\n",
        "BATCH_NUMBER=int(round(TRAINING_NUMBER/50))\n",
        "VALIDATION_NUMBER=int(BATCH_NUMBER/4)\n",
        "hyper_ds_train=ds_for_training.take(BATCH_NUMBER-VALIDATION_NUMBER)\n",
        "hyper_ds_validation=ds_for_training.skip(BATCH_NUMBER-VALIDATION_NUMBER)\n",
        "hyper_ds_validation=hyper_ds_validation.unbatch().batch(VALIDATION_NUMBER)\n",
        "\n",
        "best_loss=1\n",
        "\n",
        "\n",
        "for num_layer_dense in range(1,3):\n",
        "  for num_node_layer in range(1,3):\n",
        "    for drop_value in range(2):\n",
        "      model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(32, 32 ,3)),\n",
        "        tf.keras.layers.MaxPooling2D()])\n",
        "    \n",
        "      model.add(tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.MaxPooling2D())\n",
        "      model.add(tf.keras.layers.Dropout(drop_value/10))\n",
        "      model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.MaxPooling2D())\n",
        "      model.add(tf.keras.layers.Dropout(drop_value/10))\n",
        "      model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.MaxPooling2D())\n",
        "      model.add(tf.keras.layers.Dropout(drop_value/10))\n",
        "      model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
        "      model.add(tf.keras.layers.MaxPooling2D())\n",
        "      model.add(tf.keras.layers.Dropout(drop_value/10))\n",
        "    \n",
        "      model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "      for i in range(num_layer_dense):\n",
        "        model.add(tf.keras.layers.Dense(256*num_node_layer, activation='relu'))\n",
        "        model.add(tf.keras.layers.Dense(256*num_node_layer, activation='relu'))\n",
        "\n",
        "      tf.keras.layers.Dense(10)\n",
        "\n",
        "      model.compile(optimizer='adam',\n",
        "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "      model.fit(x=hyper_ds_train,\n",
        "                epochs=2)\n",
        "\n",
        "      temp_loss=zero_one_loss(model,hyper_ds_validation)\n",
        "      if temp_loss<=best_loss:\n",
        "        best_loss=temp_loss\n",
        "        best_model=model\n",
        "\n",
        "for i in range(5):\n",
        "  best_model.fit(x=ds_for_training,\n",
        "            epochs=1)\n",
        "  \n",
        "  test_loss=zero_one_loss(best_model,ds_for_test)\n",
        "\n",
        "  acc3.append(test_loss)\n",
        "  print('epoch: ',i+1)\n",
        "  print(test_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh3HNsF97Ptl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot the results\n",
        "epochs_range = range(1,6)\n",
        "\n",
        "plt.figure(figsize=(4, 2))\n",
        "plt.plot(epochs_range, acc1, label='Test Error first network')\n",
        "plt.plot(epochs_range, acc2, label='Test Error second network')\n",
        "plt.plot(epochs_range, acc3, label='Test Error third network')\n",
        "plt.legend(loc='upper right', prop={'size': 8})\n",
        "plt.ylabel('Test error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('Test error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}